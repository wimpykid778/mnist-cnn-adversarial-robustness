
# MNIST CNN Adversarial Robustness

This project demonstrates training, evaluation, and robustness analysis of a Convolutional Neural Network (CNN) for MNIST digit classification, including experiments with data poisoning and adversarial attacks.

## Setup

1. Install dependencies:
    ```sh
    pip install -r requirements.txt
    ```
2. Download the MNIST dataset from [Kaggle](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) and place `archive.zip` in the `data/` directory.

## Key Scripts and Usage

- **Training the Baseline Model**
   - Run:
      ```sh
      python scripts/train_mnist_cnn.py
      ```
   - This script trains a CNN on the MNIST dataset, prints accuracy, loss, confusion matrix, and saves the model to `models/mnist_cnn.keras`.

- **Data Poisoning and Adversarial Sample Generation**
   - The script `scripts/data_poisoning.py` provides utility functions to poison MNIST images (e.g., by adding colored patches) and to generate adversarial samples using the Foolbox library.

- **Evaluating Model on Poisoned and Adversarial Data**
   - `scripts/evaluate_poisoned_model.py` evaluates the model’s performance on a poisoned dataset, showing the impact of data poisoning.
   - `scripts/evaluate_adversarial_model.py` evaluates the model on adversarial examples, highlighting vulnerabilities to adversarial attacks.

- **Adversarial Training and Retesting**
   - `scripts/train_with_adversarial.py` retrains the model using both clean and adversarial data, then compares performance to the baseline for robustness analysis.

## Documentation: Purpose of Files in `docs/` Directory

- **Metrics.txt**: Contains baseline test accuracy, loss, inference time, confusion matrix, and classification report for the model trained on clean data.
- **performance_after_training_with_adversarial_data.txt**: Summarizes training and evaluation results after retraining the model with both clean and adversarial data, including confusion matrices and metrics.
- **performance_evaluation_with_adversorial_model.txt**: Reports the model’s performance on adversarial samples, including accuracy, loss, inference time, confusion matrix, and classification report.
- **performance_evaluation_with_poisoned_model.txt**: Details the model’s performance on a poisoned dataset, with accuracy, loss, inference time, confusion matrix, and classification report.
- **ThreatModel_STRIDE.md**: Documents a STRIDE-based threat model for the project, outlining potential security threats and mitigations.
- **sast_report.txt**: Provides a static application security testing (SAST) report generated by Bandit, listing detected vulnerabilities in the codebase.

## Additional Notes

- All scripts are located in the `scripts/` directory.
- The project uses TensorFlow/Keras for model building and training, NumPy and Matplotlib for data handling and visualization, scikit-learn for evaluation, and Foolbox for adversarial attacks.
- For security analysis, Bandit can be run with:
   ```sh
   bandit -r scripts/
   ```
